{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/crunchdao/adialab-notebooks/blob/main/quickstarter_notebook.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWkyd7l6wBse"
      },
      "source": [
        "# ![title](https://cdn.discordapp.com/attachments/692035498625204245/1090596888857813062/banner.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGffwE2wBsi"
      },
      "source": [
        "# Setup your crunch workspace"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wPwpdl83wBsi"
      },
      "source": [
        "#### STEP 1\n",
        "Run this cell to install the crunch library in your workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fctOAv3CwBsi"
      },
      "outputs": [],
      "source": [
        "!pip3 install crunch-cli --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4Yj1MDwBsk"
      },
      "source": [
        "#### STEP 2 \n",
        "(Temporary - will be removed once the pip package is public)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcRMNHr_wBsk"
      },
      "outputs": [],
      "source": [
        "# temporary command that will be remove once public communication done\n",
        "%env API_BASE_URL=http://api.adialab.staging.crunchdao.com\n",
        "%env WEB_BASE_URL=https://adialab.staging.crunchdao.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaVIsoyowBsl"
      },
      "source": [
        "#### STEP 3\n",
        "Importing the crunch package and instantiate it to be able to access its functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHV_tK7EwBsm"
      },
      "outputs": [],
      "source": [
        "import crunch\n",
        "crunch = crunch.load_notebook(__name__)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### STEP 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# go to your submit page and copy past your setup command to access the data\n",
        "# https://adialab.staging.crunchdao.com/submit\n",
        "!crunch setup happy-mike --token l65Za9SsJiBi8pH8xPvwSfGuRY8ChynyvD2DVxoAtWOosW6p6SNtwnci6conlWW4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJqTMeLqwBsm"
      },
      "source": [
        "# The Adialab x CrunchDAO competition"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VlAzMPaIwBsm"
      },
      "source": [
        "## A code competition\n",
        "\n",
        "This competition is divided in two phases.\n",
        "\n",
        "Submission phase - 12 weeks\n",
        "\n",
        "Out-of-Sample phase - 12 weeks\n",
        "\n",
        "During the first phase the participants will submit notebook or python scripts that build the best possible model on the data proposed by the organizers. In the second phase also called [Out-of-Sample](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) (OOS) phase the participant's code will be automatically run by the platform on live market data to be evaluated on unseen data. During this phase the participants won't be able to modify their code.\n",
        "\n",
        "- There is two main interests in proceeding that way:\n",
        "\n",
        "- The participants won't be able to game or cheat in any ways which is very often the case in traditional data-science competitions.\n",
        "\n",
        "- The [overfitting](https://deliverypdf.ssrn.com/delivery.php?ID=634087103098022017102089127026118070055022030067038035066070070118003108076075122073107013020035005031116084117030102014013119017036066065011126115081078006004108029033051020066006092025091103065117104075029100098011096065096065079019015002101078070&EXT=pdf&INDEX=TRUE) of the training data will lead to a very bad performance OOS.\n",
        "\n",
        "To ensure reproducibility of your work, you will need to follow certain guidelines to participate in the competition. These guidelines will also allow our scoring system to run your code in the cloud during the OOS period without any issues.\n",
        "\n",
        "CrunchDAO is acting as a third party intermediary in this competition and will off-course never communicate the code to the organizers in any ways.\n",
        "\n",
        "## The user interface\n",
        "\n",
        "User Interfaces are recurring solutions that solve common problems. In the world of data-science and modeling, the typical interface is covered by the following functions:\n",
        "\n",
        "1. **import**: As any script, if your solution contains dependancies on external packages make sure to import. The system will automatically your dependancies. Make sure that you are using only packages that are whitelisted in overview >> Libraries page.\n",
        "\n",
        "2. **data processing**: In the data processing step users will proceed with the transformation of the data that they deem necessary before training a model. This step includes feature selection, data transformations, creation of new synthetic features etc... This step must return the x_train, y_train and x_test data sample.\n",
        "\n",
        "3. **train**: In the training phase the users will build the model and train it such that it can perform inferences on the testing data. This function should return a trained model ready to perform inferences on the testing data.\n",
        "\n",
        "4. **infer**: In the inference function the model train in the previous step will be used to perform inferences on a data sample matching the characteristic of the training test.\n",
        "\n",
        "## Scoring on the public leaderboard\n",
        "\n",
        "To make sure that the public leaderboard is solid you don't have access to the all testing data on wich you will be scored.\n",
        "The x_test data downloaded in your workspace is composed of only 5 dates for you to test localy your code.\n",
        "Once you will have push your solution the system will run your code on a private test set of around 30 dates.\n",
        "You are left to decide how many retrain you can do under the 5 hours of ressources / week / user allowed to predict the 30 moons of the private test set.\n",
        "\n",
        "```python\n",
        "for date in dates: # This loop over private test set dates to avoids leaking the x of future periods\n",
        "\n",
        "    # The wrapper will block the logging of users code after the 5 first dates\n",
        "    if date >= log_treshold:\n",
        "        log = False\n",
        "    \n",
        "    # Cutting the sample such that the users code will only access the right part of the data\n",
        "    X_train = X_train[X_train.date < date - embargo]\n",
        "    y_train = y_train[y_train.date < date - embargo]\n",
        "    x_test = x_test[x_test.date == date] # Only the current date\n",
        "\n",
        "    # Call user interface and instantiate\n",
        "    data_process(x_train, y_train, x_test)\n",
        "\n",
        "    # The backend decide if we call train model for ALL user\n",
        "    if retrain:\n",
        "        train(x_train, y_train, model_directory_path) # This function is saving the new state of the model\n",
        "    \n",
        "    # The backend call the inference\n",
        "    prediction_current = infer(model_directory_path, X_test)\n",
        "\n",
        "    # Concat current date prediction with previous date prediction if over date log_treshold so scoring only happends after the logs are deactivated\n",
        "    if date > log_treshold:\n",
        "        prediction = pd.concat([prediction, prediction_current])\n",
        "    \n",
        "# Backend upload predictions and model_directory_path's content\n",
        "# Backend score\n",
        "```\n",
        "\n",
        "## Scoring on the out-of-sample phase\n",
        "\n",
        "During the out-of-sampple the backend will call your code 3 time every week on live datapoint.\n",
        "\n",
        "The mean spearman score after 12 weeks of OOS will determine the winners of the tournament.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwxiUFGAwBsq"
      },
      "source": [
        "# Construction of a basic submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o1AvFK_wBso"
      },
      "source": [
        "### Submission process\n",
        "\n",
        "1- Make sure to put all your code in the code interface inside your Notebook. The system will parse these functions to execute it in the cloud. You can work outside of the code interface but to be able to submit you will need to fill-in the submission function with the code you want to submit\n",
        "\n",
        "2- Once satisfied with your work. Download this notebook ( file -> Download -> Download.ipynb )\n",
        "\n",
        "3- Then upload this Notebook on https://adialab.staging.crunchdao.com/submit#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is a basic example of what you need to do to participate to the tournament.\n",
        "The code will not have access to internet (or any socket related operation) so don't try to get access to externall ressources.\n",
        "\"\"\"\n",
        "\n",
        "# Imports\n",
        "import xgboost as xgb\n",
        "import sklearn\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import typing\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "\n",
        "def scorer(y_test: pd.DataFrame, y_pred: pd.DataFrame) -> None:\n",
        "    score = (stats.spearmanr(y_test, y_pred)*100)[0]\n",
        "    print(f\"In sample spearman correlation {score}\")\n",
        "\n",
        "\n",
        "def data_process(x_train: pd.DataFrame, y_train: pd.DataFrame, x_test: pd.DataFrame) -> typing.Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Do your data processing here.\n",
        "    During the execution of your code server side this function will be executed for each date.\n",
        "\n",
        "    Args:\n",
        "        x_train (pd.DataFrame): the independant variable up to the current date minus an embargo of xx moons.\n",
        "        y_train (pd.DataFrame): the dependant data up to the current date minus an embargo of xx moons.\n",
        "        x_test (pd.DataFrame): the independant variable of the last time cross-section at each execution.\n",
        "\n",
        "    Returns:\n",
        "        (x_train, y_train, x_test) (pd.DataFrame): the dataframe passed in variable after user-processing and feature engeneering.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Starting - data_process\")\n",
        "    \n",
        "\n",
        "    return x_train, y_train, x_test\n",
        "\n",
        "\n",
        "def train(x_train: pd.DataFrame, y_train: pd.DataFrame, model_directory_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Do your model training here.\n",
        "    At each retrain this function will save an updated version of the model under the model_directiory_path.\n",
        "    Make sure to use the correct operator to read and/or write your model.\n",
        "    \n",
        "    Args:\n",
        "        x_train, y_train: the data post user processing and user feature engeneering done in the data_process function.\n",
        "        model_directory_path: the path to the directory to the directory in wich we will saving your updated model\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    # spliting training and test set\n",
        "    print(\"spliting...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # very shallow xgboost regressor\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        max_depth=4,\n",
        "        learning_rate=0.01,\n",
        "        n_estimators=2,\n",
        "        n_jobs=-1,\n",
        "        colsample_bytree=0.5\n",
        "    )\n",
        "\n",
        "    # training the model\n",
        "    print(\"fiting...\")\n",
        "    model.fit(X_train.iloc[:,2:], y_train.iloc[:,2:])\n",
        "\n",
        "    # testing model's Spearman score\n",
        "    pred = model.predict(X_test.iloc[:,2:])\n",
        "    scorer(y_test.iloc[:,2:], pred)\n",
        "\n",
        "    # make sure that the train function correctly save the trained model in the model_directory_path\n",
        "    joblib.dump(model, os.path.join(model_directory_path, \"model.joblib\"))\n",
        "\n",
        "\n",
        "def infer(model_directory_path: str, x_test: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Do your inference here.\n",
        "    This function will load the model saved at the previous iteration and use it to produce your inference on the current date.\n",
        "    It is mandatory to send your inferences with the ids so the system can match it correctly.\n",
        "    \n",
        "    Args:\n",
        "        model_directory_path: the path to the directory to the directory in wich we will be saving your updated model.\n",
        "        x_test: the independant  variables of the current date passed to your model.\n",
        "\n",
        "    Returns:\n",
        "        A dataframe with the inferences of your model for the current date, including the ids columns.\n",
        "    \"\"\"\n",
        "\n",
        "    # loading the model saved by the train function at previous iteration\n",
        "    model = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n",
        "    \n",
        "    # creating the predicted label dataframe without omiting to keep the ids and data\n",
        "    predicted = x_test[[\"date\", \"id\"]].copy()\n",
        "    predicted[\"value\"] = model.predict(x_test.iloc[:, 2:])\n",
        "\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the data\n",
        "x_train, y_train, x_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the process_data like in the backend\n",
        "x_train, y_train, x_test = data_process(x_train, y_train, x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the train function like in the backend. Please specify a directory in which you want to save your model\n",
        "train(x_train, y_train, crunch.model_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call the infer function\n",
        "crunch.call_infer(model_directory='.', x_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing your code locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function of the crunch package will run your code locally like it is called in the cloud (ie: one date at a time)\n",
        "# You can setup the a retraining frequency as you which. A train frequency of 2 means that the system will retrain your model every two dates.\n",
        "# Force first train means that your model will be train on the first date of the test set.\n",
        "crunch.test(force_first_train=True, train_frequency=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QIGffwE2wBsi",
        "fJqTMeLqwBsm",
        "nXacC2GmwBso",
        "SwxiUFGAwBsq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
